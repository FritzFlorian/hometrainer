"""Client Functionality for our neural network abstraction.

The neural network is abstracted into an client and server, in the way that the actual network can run as
a 'server' and that the client acts as a simple proxy/interface to use the neural network.
This helps, as we can then simply call functions on the neural network in our alpha zero code,
without worrying how it is actually executed, it's a plain function call.

The biggest drawback is, that we have to serialize/deserialize requests to the network.
This might be a problem, but can be solved by using foster deserialization like ray.
One extra bonus of decoupling the neural network is that we have no synchronisation issues and
that the network runs in it's own process, so it can use extra cpu cores without dragging down the
main program."""
import pickle
import multiprocessing
import numpy as np
import logging
import importlib
import zmq
import time
import random
import threading
import hometrainer.core as core
import hometrainer.util as util
from hometrainer.config import Configuration


class NeuralNetworkClient:
    def __init__(self, address):
        self.address = address

        self.context = None
        self.nn_server_socket = None
        self.internal_router = None

        self.input_conversion = None
        self.output_conversion = None

        # Make sure we will never get collisions. The name is arbitrary anyways,
        # as it is only used to handle concurrent use of the neural network client.
        self.internal_address = 'inproc://nn_client_router_{}_{}'.format(time.time(), random.random)

    def evaluate_game_state(self, game_state: core.GameState) -> core.Evaluation:
        """Executes a neural network to evaluate a given game state.

        Returns the evaluation generated by the network.
        Might block for a while because other evaluations are being performed."""
        # Create the evaluation to send to the NN.
        # We will apply rotations on random instances.
        evaluation = game_state.wrap_in_evaluation()
        evaluation = evaluation.convert_to_normal()
        evaluation = evaluation.apply_transformation(random.randint(0, 6))

        # Execute it using the neural network process.
        # We hand it through our internal proxy to be able to
        # map parallel execution results properly.
        input_array, target_array = self.input_conversion(evaluation)
        response = self.send_to_server(ExecutionRequest(input_array))
        evaluation = self.output_conversion(evaluation, response.output_array)

        # Undo our random rotation on the instance.
        evaluation = evaluation.undo_transformations()
        evaluation = evaluation.convert_from_normal()

        return evaluation

    def execute_training_batch(self, evaluations):
        """Executes a batch of training using the given evaluations."""
        inputs = []
        targets = []
        for evaluation in evaluations:
            input_array, target_array = self.input_conversion(evaluation, calculate_target=True)
            inputs.append(input_array)
            targets.append(target_array)

        return self.send_to_server(TrainingRequest(np.asarray(inputs), np.asarray(targets)))

    def save_weights(self):
        """Saves the current weights to a binary of it's checkpoint."""
        response = self.send_to_server(SaveWeightsRequest())
        return response.checkpoint_data

    def load_weights(self, checkpoint_zip_binary):
        """Loads a checkpoint from the binary of it's checkpoint."""
        self.send_to_server(LoadWeightsRequest(checkpoint_zip_binary))

    def shutdown_server(self):
        return self.send_to_server(ShutdownRequest())

    def send_to_server(self, message):
        """Sends a message to the NN Server and returns its response message.

        This method is thread save and will block each thread until the individual answer
        has been returned by the server (e.g. for execution requests this can take until
        a full batch was executed)"""
        client = self.context.socket(zmq.REQ)
        client.connect(self.internal_address)

        client.send_pyobj(message)
        response = client.recv_pyobj()

        client.close()

        return response

    def start(self, config: Configuration):
        self.context = zmq.Context()

        self.nn_server_socket = self.context.socket(zmq.DEALER)
        util.secure_client_connection(self.nn_server_socket, self.context, config)
        self.nn_server_socket.connect(self.address)

        self.internal_router = self.context.socket(zmq.ROUTER)
        self.internal_router.bind(self.internal_address)

        def run_proxy():
            try:
                zmq.proxy(self.internal_router, self.nn_server_socket)
            except zmq.ZMQError:
                pass  # The closed proxy is on purpose, so ignore the error.

        threading.Thread(target=run_proxy).start()

        # Get the conversion methods
        response = self.send_to_server(ConversionFunctionRequest())
        self.input_conversion = response.input_conversions
        self.output_conversion = response.output_conversions

    def stop(self):
        self.internal_router.setsockopt(zmq.LINGER, 0)
        self.internal_router.close()
        self.nn_server_socket.setsockopt(zmq.LINGER, 0)
        self.nn_server_socket.close()
        self.context.term()


def start_nn_server(port, nn_class_name, config: Configuration, nn_init_args=(), batch_size=32, log_dir=None, start_batch=0):
    """Starts a neural network server on the given port.

    The server is started using the venv cpython executable.
    The nn_class_name must be the fully qualified name of the neural network class.
    This method can be used from any python interpreter, for example also from
    the pypy interpreter, as all tensorflow components are loaded in the new process."""
    logging.info('Starting NN Server on port {}.'.format(port))
    p = multiprocessing.Process(target=_start_nn_server_internal,
                                args=(port, nn_class_name, batch_size, log_dir, start_batch, nn_init_args, config))
    p.start()
    return p


def _start_nn_server_internal(port, nn_class_name, batch_size, log_dir, start_batch,
                              nn_init_args, config: Configuration):
    import hometrainer.nn_server as nn_server

    module_name, class_name = nn_class_name.rsplit('.', 1)
    nn_module = importlib.import_module(module_name)
    nn_class = getattr(nn_module, class_name)

    server = nn_server.NeuralNetworkServer(port, nn_class(*nn_init_args), batch_size, config, start_batch=start_batch)
    server.run()


# Message types used to communicate with the NNServer
class AbstractMessage:
    pass


class Response(AbstractMessage):
    def __init__(self, response_ids):
        self.response_ids = response_ids

    def to_multipart(self):
        return self.response_ids + [b'', pickle.dumps(self)]


class ShutdownRequest(AbstractMessage):
    pass


class SaveWeightsRequest(AbstractMessage):
    pass


class SaveWeightsResponse(Response):
    def __init__(self, response_ids, checkpoint_data):
        super().__init__(response_ids)
        self.checkpoint_data = checkpoint_data


class LoadWeightsRequest(AbstractMessage):
    def __init__(self, checkpoint_data):
        self.checkpoint_data = checkpoint_data


class ConversionFunctionRequest(AbstractMessage):
    pass


class ConversionFunctionResponse(Response):
    def __init__(self, response_ids, input_conversion, output_conversion):
        super().__init__(response_ids)
        self.input_conversions = input_conversion
        self.output_conversions = output_conversion


class ExecutionRequest(AbstractMessage):
    def __init__(self, input_array):
        self.input_array = input_array


class ExecutionResponse(Response):
    def __init__(self, response_ids, input_array):
        super().__init__(response_ids)
        self.input_array = input_array
        self.output_array = None

    def set_output(self, output_array):
        self.output_array = output_array
        self.input_array = None


class TrainingRequest(AbstractMessage):
    def __init__(self, input_arrays, target_arrays):
        self.input_arrays = input_arrays
        self.target_arrays = target_arrays
